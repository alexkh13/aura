{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¥ Extracting Clothing Items from an Image\n",
    "\n",
    "This notebook demonstrates how to use a pre-trained computer vision model to detect and extract clothing items from an image. \n",
    "\n",
    "We will use:\n",
    "- `transformers` (from Hugging Face) to load a powerful pre-trained model.\n",
    "- `Pillow` (PIL) to handle image manipulation, drawing boxes, and cropping.\n",
    "- `requests` to download a sample image from the web.\n",
    "\n",
    "The model we'll use is `vener-a/yolos-fashion-detection`, which is fine-tuned on the Fashionpedia dataset and can recognize dozens of clothing categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, we need to install the required Python libraries. `torch` is the deep learning framework, `transformers` gives us the model, and `Pillow` & `requests` are for image handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Processor\n",
    "\n",
    "Every model in the `transformers` library comes with two parts:\n",
    "1.  **Processor:** This formats the image (resizes, normalizes colors) so it's in the exact format the model expects.\n",
    "2.  **Model:** This is the pre-trained neural network that performs the object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the model name from the Hugging Face Hub\n",
    "model_name = \"vener-a/yolos-fashion-detection\"\n",
    "\n",
    "# Load the processor and the model\n",
    "print(\"Loading model and processor...\")\n",
    "processor = YolosImageProcessor.from_pretrained(model_name)\n",
    "model = YolosForObjectDetection.from_pretrained(model_name)\n",
    "print(\"Model and processor loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load a Sample Image\n",
    "\n",
    "Let's grab a sample image from the web. You can easily replace the `image_url` with a path to your own local file (e.g., `image = Image.open('my_photo.jpg')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for a sample image. Feel free to change this!\n",
    "image_url = \"https://images.pexels.com/photos/1036627/pexels-photo-1036627.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\"\n",
    "\n",
    "# Download and open the image\n",
    "try:\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    \n",
    "    # Display the original image\n",
    "    print(\"Original Image:\")\n",
    "    display(image.resize((400, 600))) # Resize for neat display in notebook\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading image: {e}\")\n",
    "    print(\"Please check the URL or try a different one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Detection\n",
    "\n",
    "Now we pass the image through the processor and then the model to get the raw predictions (logits and bounding boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the processed image to the model\n",
    "print(\"Running detection...\")\n",
    "outputs = model(**inputs)\n",
    "print(\"Detection complete.\")\n",
    "\n",
    "# The model outputs raw logits and bounding boxes\n",
    "# We'll process these in the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-Process and Visualize Results\n",
    "\n",
    "The raw outputs from the model are numbers. The `processor` has a built-in function (`post_process_object_detection`) to convert these numbers into meaningful labels and coordinates. \n",
    "\n",
    "We'll set a `threshold` to filter out low-confidence detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the original image size to scale the bounding boxes correctly\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "\n",
    "# Post-process the outputs. We set a confidence threshold.\n",
    "confidence_threshold = 0.9\n",
    "results = processor.post_process_object_detection(outputs, threshold=confidence_threshold, target_sizes=target_sizes)[0]\n",
    "\n",
    "# Create a copy of the image to draw on\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Loop over all detected objects\n",
    "for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    # Get the bounding box coordinates\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    \n",
    "    # Get the human-readable label\n",
    "    label = model.config.id2label[label_id.item()]\n",
    "    \n",
    "    # Format the score\n",
    "    score_str = f\"{score*100:.1f}%\n",
    "    \n",
    "    print(f\"Detected: {label} (Confidence: {score_str})\")\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    draw.rectangle(box, outline=\"red\", width=3)\n",
    "    \n",
    "    # Draw the label and score\n",
    "    # Note: This doesn't use a specific font, so it will be basic.\n",
    "    draw.text((box[0], box[1]), f\"{label} ({score_str})\", fill=\"red\")\n",
    "\n",
    "print(\"\\nImage with Bounding Boxes:\")\n",
    "display(image_with_boxes.resize((400, 600)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \"Extract\" (Crop) the Detected Items\n",
    "\n",
    "Now that we have the coordinates (`box`) for each item, we can use Pillow's `crop()` function to extract each item into its own separate image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Extracting {len(results['scores'])} items...\\n\")\n",
    "\n",
    "for i, (score, label_id, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
    "    # Get the label name\n",
    "    label = model.config.id2label[label_id.item()]\n",
    "    \n",
    "    # Get the coordinates\n",
    "    box_coords = [round(c) for c in box.tolist()]\n",
    "    \n",
    "    # Crop the image using the bounding box\n",
    "    cropped_item = image.crop(box_coords)\n",
    "    \n",
    "    print(f\"--- Item {i+1}: {label} ---\")\n",
    "    display(cropped_item.resize((150, 150))) # Resize for neat display\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook demonstrated the complete pipeline for clothing detection:\n",
    "1.  **Installed** `transformers` and `pillow`.\n",
    "2.  **Loaded** a pre-trained fashion detection model (`YolosForObjectDetection`).\n",
    "3.  **Processed** a sample image.\n",
    "4.  **Ran** the model to get predictions.\n",
    "5.  **Visualized** the results by drawing bounding boxes.\n",
    "6.  **Extracted** each item by cropping it from the original image.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Try your own images!** Just change the `image_url` variable to a local file path.\n",
    "- **Adjust the `confidence_threshold`:** A lower value (e.g., `0.7`) will show more detections, but they might be less accurate. A higher value (e.g., `0.95`) will show only very confident detections.\n",
    "- **Explore segmentation:** This model performs *object detection* (boxes). More advanced models perform *instance segmentation*, which creates a pixel-perfect mask around each item. You can look for models like Mask R-CNN for that task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
